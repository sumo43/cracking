


Stock Data

- Store the data in a table, with the stock identifier, name, high, low, etc
- Each customer has a table of stocks associated with him/her
- Each day, the apps send a request for the stock information table, for the stocks associated with the customer
- Load balancer that distributes requests by category
- The stocks are stored by category, which is included in the customer request, but some machines may have more than 1 category if they are small enough


No SQL Implementation
- Each stock is an object with a name, high, low, history, etc.
    - Can be updated throughout the day
- Each stock has a list of person objects who want the stock sent to them. 
    - The user application also has a copy of the person object, which has a reference to the stock
        - If the userwants more info, this reference can be used to access the stock quickly

Social Network

Shortest-Path Algorithm
Algorithm: Breadth First Search

How to make it Efficient:
    - Keep a list of Friends in a noSQL database, like Firebase or MongoDB
        - Each Friend object has a name, address, and list of references to other Friend objects


SQL Version
    - Keep a table of users, with each user having a table of Friends to be searched through
    - Keep the recently/frequently looked-at or added friend indices in short term memory/cache for fast access


Web Crawler

Is there only one web crawler? Does access speed matter, if using database


Solution 1:
- Keep a hash/dictionary of the recent websites of the web crawler, which deletes the last entry every 15mins
- Select a different index link every time the web crawler reaches an index

Duplicate URLs:
   

- Is this a one-time operation? Do the URLs update over time?


Start with a server for each first letter (assuming all of the addresses have an equal probability, we end up with about 300 million URLs per server)

For each second letter, create a table for each second letter, this leaves you with about 10 million URLs per table
For each URL in the table, use local memory to match the URLs letter by letter, 300,000 at a time

Cache

Each machine can send a request getCache(string query) to a hash mapping on an 11th server.
If the query is not in the hash, the server uses processSearch
- This uses a rolling representation of the most frequent queries accross the user base
- Queries are deleted if their popularity score (number of users with the query / time in server) drops below a certain score

Sales Rank

keep a table of products (ordered by their overall index), and X tables of categories, with each item being ordered by its category index
- The two tables are linked by the item id (or the name)


Every 15 minutes, a script would check the order of each item and its number of sales (in order), and swap the indices of any two orders when their number of sales changes
- The script would run several times if necessary
k
- The script would keep a hash of categories. Whenever a ranking of an item changes, the categories of the (two) changed items would be added to the hash
- Any categories with nonzero hash entries would be iterated the same way as the main list

Personal Financial Manager

- The application will be read-heavy, so we can use a SQL database
- There would be a table labeled "transactions" ordered chronologically (can be used for analytics) with properties such as people, price, time, card type
- There would also be a table labeled "people" which would store people/companies in transactions. Each people id (from the main table) can have more than one person/company associated with it. This can also be used for analytics
- There will be a table for each category, with the category being inferred when the transaction entry is created based on a separate database of person/company to category mappings
- Each time you open the app, the app sends requests for recent transactions, categories, and people, and a request for recommendations if there are less than 3 recommendations visible. 
    - Each recommendation is based on the most shopped category of the customer

Pastebin

- I won't tackle the analytics part of the problem
- URLs will be generated using a UUID-type algorithm, while checking if the URLs exist will be handled by a nosql database with URL hashed
    - The NoSQL database will also have the amount of users/traffic tied to a cloud function, which decides whether to allocate more memory to the site or not




